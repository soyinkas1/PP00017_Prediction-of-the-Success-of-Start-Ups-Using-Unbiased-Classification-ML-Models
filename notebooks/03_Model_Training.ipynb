{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0c630a",
   "metadata": {},
   "source": [
    " ### PREDICTION OF THE SUCCESS OF START-UPS USING UNBIASED CLASSIFICATION MACHINE LEARNING MODELS \n",
    " \n",
    "**Problem Statement**\n",
    "This notebook contains the final part of the research work to build a supervised learning classification machine learning model that facilitates the prediction of the success of a company based on situational informtion only available at the point of seeking financing by the company.. \n",
    "\n",
    "**Data**\n",
    "Historical data about companies was sourced from Crunchbase. Crunchbase tracks and collects information about millions of companies and related personnel. Past research works has been carried out using Crunchbase data but features such as company descriptions and people descriptions and other textual features were not used for the model buiding. To further enrich the dataset information from social media was scrapped and analysed for sentiments, however due to Twitter restrictions and unrealistic resources required to scrape for all companies, the Twitter scrapping code though available is not utilised for the final dataset. Natural Language Processing of the organisation description and people description were carried out as will as features engineering of additional features from the raw dataset.\n",
    "\n",
    "**Models**\n",
    "\n",
    "The following  ML models will be experimented on with the final dataset.\n",
    "* Logistic Regression\n",
    "* RandomForest Classifiers\n",
    "* SVM Classifers\n",
    "* Naives Bayes\n",
    "* LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc3c56",
   "metadata": {},
   "source": [
    " ### Machine Learning Model\n",
    " \n",
    "**Scikit Learn ML Library will be utlised and the following steps carried**\n",
    " \n",
    "  * upload the preprocessed train, validation and test datasets\n",
    "  * train models and evaluate with validation set\n",
    "  * Carry out hyperparameter tuning as required.\n",
    "  * Evaluate and select the best model using the test data.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data Analysis Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dill\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Import Machine Learning Classifiers models Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Import evaluations modules\n",
    "#from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "\n",
    "# Import other needed machine Learning Libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = pd.read_csv(\"D:\\\\OneDrive\\\\Documents\\\\Personal Project Portfolio\\\\PP00017_Prediction of the Success of Start-Ups\\\\artifacts\\\\data_transformation\\\\train_data.csv\", chunksize=3000, low_memory=True)\n",
    "val_reader = pd.read_csv(\"D:\\\\OneDrive\\\\Documents\\\\Personal Project Portfolio\\\\PP00017_Prediction of the Success of Start-Ups\\\\artifacts\\\\data_transformation\\\\validate_data.csv\", chunksize=3000, low_memory=True)\n",
    "test_reader = pd.read_csv(\"D:\\\\OneDrive\\\\Documents\\\\Personal Project Portfolio\\\\PP00017_Prediction of the Success of Start-Ups\\\\artifacts\\\\data_transformation\\\\test_data.csv\", chunksize=3000, low_memory=True)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "train = pd.concat(train_reader)\n",
    "val = pd.concat(val_reader)\n",
    "test = pd.concat(test_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the training, validation and test dataset\n",
    "\n",
    "train = pd.read_csv(\"D:\\OneDrive\\Documents\\Personal Project Portfolio\\PP00017_Prediction of the Success of Start-Ups\\artifacts\\data_transformation\\train_data.csv\")\n",
    "val = pd.read_csv(\"D:\\OneDrive\\Documents\\Personal Project Portfolio\\PP00017_Prediction of the Success of Start-Ups\\artifacts\\data_transformation\\validate_data.csv\")\n",
    "test = pd.read_csv(\"D:\\OneDrive\\Documents\\Personal Project Portfolio\\PP00017_Prediction of the Success of Start-Ups\\artifacts\\data_transformation\\test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164324fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dce93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train.iloc[:, :-1],train.iloc[:, -1], val.iloc[:, :-1], val.iloc[:, -1],test.iloc[:, :-1], test.iloc[:, -1]\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the dimensions of the train and test dataset\n",
    "print (X_train.shape,y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score_best_model(models,preprocessor_path, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "    X_train : training data (without labels)\n",
    "    X_test : test data (without labels)\n",
    "    y_train : training labels\n",
    "    y_test : test labels\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    The best performing model out of the model passed to it\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Make a dictionary to keep model scores\n",
    "    model_report = {}\n",
    "        \n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "       \n",
    "        # Evaluate the model and append its score to model_report\n",
    "        model_report[name] = model.score(X_test, y_test)\n",
    "        \n",
    "    # To get best model score from dict\n",
    "    best_model_score = max(sorted(model_report.values()))\n",
    "      \n",
    "    # To get best model name from dict\n",
    "    best_model_name = list(model_report.keys())[\n",
    "                list(model_report.values()).index(best_model_score)\n",
    "            ]\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    if best_model_score < 0.6:\n",
    "        raise CustomException(\"No best model found\")\n",
    "           \n",
    "#     logging.info(f\"Best found model on both training and testing dataset\")\n",
    "\n",
    "#     save_object(\n",
    "#                 file_path=self.model_trainer_config.trained_model_file_path,\n",
    "#                 obj=best_model\n",
    "#             )\n",
    "\n",
    "    return print(f'Best Model: {best_model}, Score: {best_model_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48017d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put models in a dictionary\n",
    "models = {'Logistic Regression':LogisticRegression(),'SVM': SVC(),'Random Forest': RandomForestClassifier(),\n",
    "           'XGB': XGBClassifier(), 'LightGBM': lgb.LGBMClassifier() }\n",
    "\n",
    "best_model = fit_and_score_best_model(models,\"D:\\\\OneDrive\\\\Documents\\\\PERSONAL\\\\PERSONAL DEVELOPMENT\\\\DATA SCIENCE\\\\Personal Project Portfolio\\\\PP00017_Prediction of the Success of Start-Ups Using Unbiased Classification ML Models\\\\artifacts\\\\data_transformation\\\\preprocessor.pkl\",\n",
    "                                      X_train=X_train, X_test=X_val,y_train=y_train, y_test=y_val)\n",
    "\n",
    "best_model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_object(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file_obj:\n",
    "            return dill.load(file_obj)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise CustomException(e, sys)\n",
    "        \n",
    "        \n",
    "def save_object(file_path, obj):\n",
    "    with open(file_path, 'wb') as file_obj:\n",
    "        dill.dump(obj, file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add helper function with hyperparameter tuning with GridSearch CV\n",
    "def evaluate_models(X_train, y_train, X_test, y_test, models, param):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This method fits and score the models provided while doing a gridsearch cross\n",
    "        validation using the parameter grid provided\n",
    "        \n",
    "        input: X-train - Training data input features\n",
    "             y_train - Training data label \n",
    "             X_test - Test data input features\n",
    "             y_test - Test data labels\n",
    "             models - ML model to experiment with\n",
    "             param :dict - parameter settings to try as values.\n",
    "             \n",
    "        Returns: a dictionary of the a key values pair of model and score\n",
    "        \"\"\" \n",
    "                \n",
    "        report = {}\n",
    "\n",
    "        for i in range(len(list(models))):\n",
    "            model = list(models.values())[i]\n",
    "            para=param[list(models.keys())[i]]\n",
    "\n",
    "            gs = GridSearchCV(model,para,cv=3, verbose=3)\n",
    "            gs.fit(X_train,y_train)\n",
    "\n",
    "            model.set_params(**gs.best_params_)\n",
    "            model.fit(X_train,y_train)\n",
    "\n",
    "            #model.fit(X_train, y_train)  # Train model\n",
    "\n",
    "#             y_train_pred = model.predict(X_train)\n",
    "\n",
    "#             y_test_pred = model.predict(X_test)\n",
    "\n",
    "            test_model_score = model.score(X_test, y_test)\n",
    "\n",
    "            report[list(models.keys())[i]] = test_model_score\n",
    "        return report\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the fit_score_best_model helper function\n",
    "\n",
    "def fit_and_score_best_model(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "    X_train : training data (without labels)\n",
    "    X_test : test data (without labels)\n",
    "    y_train : training labels\n",
    "    y_test : test labels\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    The best performing model out of the model passed to it\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create the parameter grid\n",
    "    params={\n",
    "                'Logistic Regression': {\n",
    "                    'penalty':[None, '12', 'l1', 'elasticnet'],\n",
    "                    'dual':[True,False],\n",
    "                    'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "                },\n",
    "                'SVM':{\n",
    "                    'C':[0.1, 1, 10, 1000],\n",
    "                    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                    'degree':[0, 1, 2, 3, 4, 5, 6],\n",
    "                    'gamma':['scale', 'auto'],\n",
    "                  \n",
    "                },\n",
    "              'Random Forest': {\n",
    "                      'criterion': ['gini', 'entropy'],\n",
    "                      'max_features': ['sqrt','log2'],\n",
    "                      'n_estimators': [8, 32, 128, 256],\n",
    "                      'max_leaf_nodes': [None, 16, 32 ],\n",
    "                      'bootstrap': [True, False],\n",
    "                      'n_jobs': [1, -1],\n",
    "                      'max_samples': [2, 4,6],\n",
    "                      'min_samples_split': [2, 6, 10, 14, 18],\n",
    "                      'min_samples_leaf': [1, 5, 9, 13, 17]\n",
    "              },\n",
    "                'XGB':{\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'max_depth': [6, 8, 10],\n",
    "                    'gamma': [2, 4, 9, 12],\n",
    "                    'sampling_method': ['uniform', 'gradient_based'],\n",
    "                    'grow_policy': ['depthwise', 'lossguide'],\n",
    "                    'n_estimators': [8,16,32,64,128,256]\n",
    "                    \n",
    "                },\n",
    "                'LightGBM':{\n",
    "                    'boosting_type':['gbdt', 'rf', 'dart'],\n",
    "                    'max_depth': [-1, 2, -10],\n",
    "                    'learning_rate': [0.01, 0.05, 0.1],\n",
    "                    'n_estimator': [100, 50, 200],\n",
    "                    'num_leaves': [31, 50, 100]\n",
    "                },\n",
    "    }\n",
    "          \n",
    "    # Evaluate the model and append its score to model_report\n",
    "    model_report:dict=evaluate_models(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
    "                                             models=models, param=params)\n",
    "        \n",
    "    # To get best model score from dict\n",
    "    best_model_score = max(sorted(model_report.values()))\n",
    "      \n",
    "    # To get best model name from dict\n",
    "    best_model_name = list(model_report.keys())[\n",
    "                list(model_report.values()).index(best_model_score)\n",
    "            ]\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    if best_model_score < 0.6:\n",
    "        raise CustomException(\"No best model found\")\n",
    "           \n",
    "#     logging.info(f\"Best found model on both training and testing dataset\")\n",
    "\n",
    "    save_object(\n",
    "                file_path=\"D:\\\\OneDrive\\\\Documents\\\\PERSONAL\\\\PERSONAL DEVELOPMENT\\\\DATA SCIENCE\\\\Personal Project Portfolio\\\\PP00017_Prediction of the Success of Start-Ups Using Unbiased Classification ML Models\\\\artifacts\\\\models\\model.pkl\",\n",
    "                obj=best_model\n",
    "            )\n",
    "\n",
    "    return print(f'Best Model: {best_model}, Score: {best_model_score}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put models in a dictionary\n",
    "models = {'Logistic Regression':LogisticRegression(),'SVM': SVC(),'Random Forest': RandomForestClassifier(),\n",
    "           'XGB': XGBClassifier(), 'LightGBM': lgb.LGBMClassifier() }\n",
    "\n",
    "best_model = fit_and_score_best_model(models, X_train=X_train, X_test=X_val,\n",
    "                                      y_train=y_train, y_test=y_val)\n",
    "\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6154a",
   "metadata": {},
   "source": [
    "Now we have a baseline model. So will tue the hyperparameters and have more evaluation metrics for review:\n",
    "* Hyperparamenter tuning\n",
    "* Feature Importance\n",
    "* Confusion Matrix\n",
    "* Cross-validation\n",
    "* Precision\n",
    "* Recall\n",
    "* F1 Score\n",
    "* Classification Report\n",
    "* ROC curve\n",
    "* Area under the C curve\n",
    "\n",
    "To also experiment on the dataset composition, we will create versions of the dataset as follows:\n",
    "\n",
    "1. full-data (numeric , categorical & text)\n",
    "2. medium_1_data (numeric and categories (education & location & biz category & job details))\n",
    "3. medium_2_data (numeric and categories (education & biz category))\n",
    "4. medium_3-data (numeric and categories (biz category only)\n",
    "5. no gender_data (full data without gender)\n",
    "6. num_data (numeric only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765937d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for full data\n",
    "def fit_and_score_fd(dataset, models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it based on the data structure\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A dictionary of accuracy scores of each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define preprocessors\n",
    "\n",
    "    # pipeline for text data1\n",
    "    text1_features = 'short_description_o'\n",
    "    text1_transformer = Pipeline(steps=[\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "    ])\n",
    "\n",
    "    # pipeline for text data2\n",
    "    text2_features = 'description_o'\n",
    "    text2_transformer = Pipeline(steps=[\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "    ])\n",
    "\n",
    "    # pipeline for text data3\n",
    "    text3_features = 'description_o'\n",
    "    text3_transformer = Pipeline(steps=[\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "    ])\n",
    "\n",
    "    # pipeline for categorical data\n",
    "\n",
    "    categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                           'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline for numeric data\n",
    "    numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                         'degree_length','employee_count_min','employee_count_max']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"tex1\", text1_transformer, text1_features),\n",
    "            (\"tex2\", text2_transformer, text2_feat\n",
    "             ures),\n",
    "            (\"tex3\", text3_transformer, text3_features),\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ])\n",
    "\n",
    "    X = dataset.drop('success',axis=1)\n",
    "    y = dataset['success']\n",
    "\n",
    "    # Split into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Make a dictionary to keeo model scores\n",
    "    model_score = {}\n",
    "\n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "\n",
    "        # Fit the model to the data\n",
    "        clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        model_score[f'fd_{name}'] = f'{clf.score(X_test, y_test):.3f}%'\n",
    "\n",
    "\n",
    "#     model_score_df =pd.DataFrame(model_score)\n",
    "\n",
    "    return model_score #model_score_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6206af92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_and_score_fd(full_data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_1_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783553c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for medium_1_data\n",
    "def fit_and_score_m1d(dataset, models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it based on the data structure\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A dictionary of accuracy scores of each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define preprocessors\n",
    "\n",
    "\n",
    "    # pipeline for categorical data\n",
    "\n",
    "    categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                           'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline for numeric data\n",
    "    numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                         'degree_length','employee_count_min','employee_count_max']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ])\n",
    "\n",
    "    X = dataset.drop('success',axis=1)\n",
    "    y = dataset['success']\n",
    "\n",
    "    # Split into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Make a dictionary to keeo model scores\n",
    "    model_score = {}\n",
    "\n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "\n",
    "        # Fit the model to the data\n",
    "        clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        model_score[f'm1d_{name}'] = f'{clf.score(X_test, y_test)}:.3f%'\n",
    "\n",
    "    model_score = {k:[v] for k,v in model_score.items()}\n",
    "    model_score_df =pd.DataFrame(model_score)\n",
    "\n",
    "    return model_score_df.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8149db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score_m1d(medium_1_data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for medium_1_data\n",
    "def fit_and_score_m2d(dataset, models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it based on the data structure\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A dictionary of accuracy scores of each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define preprocessors\n",
    "\n",
    "\n",
    "    # pipeline for categorical data\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "    categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                           'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "    categorical_2_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline for numeric data\n",
    "    numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                         'degree_length','employee_count_min','employee_count_max']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "    X = dataset.drop('success',axis=1)\n",
    "    #X.drop(['geometry','roles','type_o','primary_role'],axis=1,inplace=True)\n",
    "    y = dataset['success']\n",
    "\n",
    "    # Split into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Make a dictionary to keeo model scores\n",
    "    model_score = {}\n",
    "\n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "\n",
    "        # Fit the model to the data\n",
    "        clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        model_score[f'm2d_{name}'] = f'{%:.3fclf.score(X_test, y_test)}%'\n",
    "\n",
    "    model_score = {k:[v] for k,v in model_score.items()}\n",
    "    model_score_df =pd.DataFrame(model_score)\n",
    "\n",
    "    return model_score_df.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score_m2d(medium_2_data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for medium_1_data\n",
    "def fit_and_score_m3d(dataset, models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it based on the data structure\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A dictionary of accuracy scores of each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define preprocessors\n",
    "\n",
    "\n",
    "    # pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "    # pipeline for categorical data\n",
    "\n",
    "    categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "    categorical_3_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline for numeric data\n",
    "    numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                         'degree_length','employee_count_min','employee_count_max']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "    X = dataset.drop('success',axis=1)\n",
    "    y = dataset['success']\n",
    "\n",
    "    # Split into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Make a dictionary to keeo model scores\n",
    "    model_score = {}\n",
    "\n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "\n",
    "        # Fit the model to the data\n",
    "        clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        model_score[f'm3d_{name}'] = f'{clf.score(X_test, y_test):.3f}%'\n",
    "\n",
    "    model_score = {k:[v] for k,v in model_score.items()}\n",
    "    model_score_df =pd.DataFrame(model_score)\n",
    "\n",
    "    return model_score_df.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score_m3d(medium_3_data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99094ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for nog data\n",
    "def fit_and_score_nog(dataset, models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it based on the data structure\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A dictionary of accuracy scores of each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define preprocessors\n",
    "\n",
    "    # pipeline for text data1\n",
    "    text1_features = 'short_description_o'\n",
    "    text1_transformer = Pipeline(steps=[\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "    ])\n",
    "\n",
    "    # pipeline for text data2\n",
    "    text2_features = 'description_o'\n",
    "    text2_transformer = Pipeline(steps=[\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "    ])\n",
    "\n",
    "    # pipeline for text data3\n",
    "    text3_features = 'description_o'\n",
    "    text3_transformer = Pipeline(steps=[\n",
    "        ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "    ])\n",
    "\n",
    "    # pipeline for categorical data\n",
    "\n",
    "    no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                           'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "    no_gender_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "\n",
    "    # pipeline for numeric data\n",
    "    numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                         'degree_length','employee_count_min','employee_count_max']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"tex1\", text1_transformer, text1_features),\n",
    "            (\"tex2\", text2_transformer, text2_features),\n",
    "            (\"tex3\", text3_transformer, text3_features),\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"nog\", no_gender_transformer, no_gender_features),\n",
    "        ])\n",
    "\n",
    "    X = dataset.drop('success',axis=1)\n",
    "    y = dataset['success']\n",
    "\n",
    "    # Split into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Make a dictionary to keeo model scores\n",
    "    model_score = {}\n",
    "\n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "\n",
    "        # Fit the model to the data\n",
    "        clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        model_score[f'nog_{name}'] = f'{clf.score(X_test, y_test):.3f}%'\n",
    "\n",
    "    model_score = {k:[v] for k,v in model_score.items()}\n",
    "    model_score_df =pd.DataFrame(model_score)\n",
    "\n",
    "    return model_score_df.T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score_nog(no_gender_data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb91922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for full data\n",
    "def fit_and_score_num(dataset, models):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits and evaluates models passed to it based on the data structure\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : Dictionary of machine learning models\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    A dictionary of accuracy scores of each model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # pipeline for numeric data\n",
    "    numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                         'degree_length','employee_count_min','employee_count_max']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "\n",
    "            (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "        ])\n",
    "\n",
    "    X = dataset.drop('success',axis=1)\n",
    "    y = dataset['success']\n",
    "\n",
    "    # Split into train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Make a dictionary to keeo model scores\n",
    "    model_score = {}\n",
    "\n",
    "    # Lopp through models\n",
    "    for name, model in models.items():\n",
    "\n",
    "        # Fit the model to the data\n",
    "        clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        model_score[f'num_{name}'] = f'{clf.score(X_test, y_test):.3f}%'\n",
    "\n",
    "    model_score = {k:[v] for k,v in model_score.items()}\n",
    "    model_score_df =pd.DataFrame(model_score)\n",
    "\n",
    "    return model_score_df.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_score_num(num_data, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb11a85",
   "metadata": {},
   "source": [
    "## We will carry out hyperparameter tuning and also evaluate for the folllowing as stated earlier:\n",
    "\n",
    "* Hyperparamenter tuning\n",
    "* Feature Importance\n",
    "* Confusion Matrix\n",
    "* Cross-validation\n",
    "* Precision\n",
    "* Recall\n",
    "* F1 Score\n",
    "* Classification Report\n",
    "* ROC curve\n",
    "* Area under the C curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use RandomizedSearchCV due to limited time to deploy Gridsearch CV\n",
    "\n",
    "# Create a hyperparameter grid for Logistic Regression\n",
    "log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'clf__C': np.logspace(-4,4,20),\n",
    "                'clf__solver':['lbfgs','liblinear'],\n",
    "               'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "# Create a hyperparameter grid for SVC\n",
    "svc_grid = {'clf__C': [0.1, 1, 10, 1000], \n",
    "           'clf__degree':[0, 1, 2, 3, 4, 5, 6],\n",
    "           'clf__kernel':['linear', 'rbf', 'poly']}\n",
    "\n",
    "\n",
    "\n",
    "# Create a hyperparameter grid for Random Forest\n",
    "rf_grid = {'clf__n_estimators': np.arange(10,1000,50),\n",
    "          'clf__max_depth': [None, 3, 5, 10],\n",
    "          'clf__min_samples_split': np.arange(2, 20, 2),\n",
    "          'clf__min_samples_leaf': np.arange(1, 20, 2)}\n",
    "\n",
    "# Create a hyperparameter grid for Naive Bayes\n",
    "nb_grid ={'clf__alpha':[0.1, 1, 10, 1000],\n",
    "          'clf__fit_prior': [True,False]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268ea32",
   "metadata": {},
   "source": [
    "### Full Data\n",
    "\n",
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eaa7d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg fd is:{model_score}')\n",
    "print(f'Best params for Log_Reg fd is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd1052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run for 'm1d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg m1d is:{model_score}')\n",
    "print(f'Best params for Log_Reg is m1d:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe441df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm2d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg m2d is:{model_score}')\n",
    "print(f'Best params for Log_Reg m2d is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg m3d is:{model_score}')\n",
    "print(f'Best params for Log_Reg m3d is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg nog is:{model_score}')\n",
    "print(f'Best params for Log_Reg nog is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg num is:{model_score}')\n",
    "print(f'Best params for Log_Reg num is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd407f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg ned is:{model_score}')\n",
    "print(f'Best params for Log_Reg ned is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2103073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg sed is:{model_score}')\n",
    "print(f'Best params for Log_Reg sed is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=log_reg_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for log_reg fed is:{model_score}')\n",
    "print(f'Best params for Log_Reg fed is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804d1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f6546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836242b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767bb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b4208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be1186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c405397",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aca0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC fd is:{model_score}')\n",
    "print(f'Best params for SVC fd is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5976b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm1d' data\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    " = medium_1_data['success']\n",
    "\n",
    "   ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC m1d is:{model_score}')\n",
    "print(f'Best params for SVC m1d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2914981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm2d' data\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC m2d is:{model_score}')\n",
    "print(f'Best params for SVC m2d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86776a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "#rand_params =[log_reg_grid,svc_grid,rf_grid,nb_grid]\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC m3d is:{model_score}')\n",
    "print(f'Best params for SVC m3d is:{clf_best}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC nog is:{model_score}')\n",
    "print(f'Best params for SVC nog is:{clf_best}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC num is:{model_score}')\n",
    "print(f'Best params for SVC num is:{clf_best}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC ned is:{model_score}')\n",
    "print(f'Best params for SVC ned is:{clf_best}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9695206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC sed is:{model_score}')\n",
    "print(f'Best params for SVC sed is:{clf_best}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=svc_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for SVC fed is:{model_score}')\n",
    "print(f'Best params for SVC fed is:{clf_best}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651a261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eafed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922bea24",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8207739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for full data\n",
    "\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF fd is:{model_score}')\n",
    "print(f'Best params for RF fd is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10431da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm1d' data\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "   ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF m1d is:{model_score}')\n",
    "print(f'Best params for RF m1d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm2d' data\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF m2d is:{model_score}')\n",
    "print(f'Best params for RF m2d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF m3d is:{model_score}')\n",
    "print(f'Best params for RF m3d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF nog is:{model_score}')\n",
    "print(f'Best params for RF nog is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec942063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF num is:{model_score}')\n",
    "print(f'Best params for RF num is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9cb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF ned is:{model_score}')\n",
    "print(f'Best params for RF ned is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF sed is:{model_score}')\n",
    "print(f'Best params for RF sed is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=rf_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for RF fed is:{model_score}')\n",
    "print(f'Best params for RF fed is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb76714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c33847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b04ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ace5be",
   "metadata": {},
   "source": [
    "**Naives Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for full data\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB fd is:{model_score}')\n",
    "print(f'Best params for NB fd is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm1d' data\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "   ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB m1d is:{model_score}')\n",
    "print(f'Best params for NB m1d is:{clf_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm2d' data\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB m2d is:{model_score}')\n",
    "print(f'Best params for NB m2d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB m3d is:{model_score}')\n",
    "print(f'Best params for NB m3d is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43690f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB nog is:{model_score}')\n",
    "print(f'Best params for NB nog is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB num is:{model_score}')\n",
    "print(f'Best params for NB num is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB ned is:{model_score}')\n",
    "print(f'Best params for NB ned is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB sed is:{model_score}')\n",
    "print(f'Best params for NB sed is:{clf_best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c19430",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB())])\n",
    "\n",
    "# log_reg_grid = {'clf__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "#                 'clf__C': np.logspace(-4,4,20),\n",
    "#                 'clf__solver':['lbfgs','liblinear'],\n",
    "#                'clf__max_iter': np.arange(100,300,50)}\n",
    "\n",
    "clf_model = RandomizedSearchCV(clf, param_distributions=nb_grid, cv=5, n_iter=20,verbose=True) \n",
    "\n",
    "clf_model.fit(X_train, y_train)\n",
    "clf_best = clf_model.best_estimator_\n",
    "\n",
    "model_score = clf_model.score(X_test, y_test)\n",
    "best_params  =  clf_best\n",
    "\n",
    "\n",
    "print(f'RandomizedSearchCV score for NB fed is:{model_score}')\n",
    "print(f'Best params for NB fed is:{clf_best}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56718ef8",
   "metadata": {},
   "source": [
    "### Final Experiments\n",
    "\n",
    "Using the best parameters from the hyperparameter tuning, we will now model and evaluate each using the evaluation techniques for each model for each of the  9 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beaa8a8",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c937598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg fd is {clf.score(X_test, y_test)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "\n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn's heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds ),\n",
    "                    annot=True,\n",
    "                    cbar=False,\n",
    "                    fmt='.0f')\n",
    "    plt.xlabel('True label')\n",
    "    plt.ylabel('Predicted label')\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab47c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9097304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f32980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa62732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_fd)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg fd is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_fd)', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ee7e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm1d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg m1d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_m1d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0092b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm2d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg m2d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_m2d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9ec9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22622ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg m3d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_m3d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c50a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg nog is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_nog)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c4488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefeea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg num is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_num)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3ac89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg ned is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_ned)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1a8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97699a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg sed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_sed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0ef54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for log_reg fed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_fed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c517beda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4508e3ae",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d86d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC fd is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_fd)', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75202ceb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e31c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm1d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC m1d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_m1d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83197706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm2d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC m2d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_m2d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137673e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41759e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC m3d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_m3d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f101949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC nog is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_nog)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fe104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=1, degree=0))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC num is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_num)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c9484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC ned is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_ned)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19537f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC sed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (SVC_sed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9304cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", SVC(C=0.1))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for SVC fed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic Reg_fed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feea9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76e54c05",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f471bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=10, min_samples_leaf=3,\n",
    "                                        min_samples_split=16,\n",
    "                                        n_estimators=710))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_fd is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_fd)', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41941796",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm1d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\",  RandomForestClassifier(max_depth=5, min_samples_split=18,\n",
    "                                        n_estimators=610))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_m1d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_m1d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370212d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee80795",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm2d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=5, min_samples_split=18,\n",
    "                                        n_estimators=610))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_SVC m2d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_m2d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8507f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3225d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=5, min_samples_split=18,\n",
    "                                        n_estimators=610))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_m3d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_m3d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359130c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb39608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=10, min_samples_leaf=3,\n",
    "                                        min_samples_split=16,\n",
    "                                        n_estimators=710))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_nog is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_nog)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01eb96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=3, min_samples_leaf=7,\n",
    "                                        min_samples_split=6,\n",
    "                                        n_estimators=560))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_num is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_num)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97a959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=10, min_samples_leaf=3,\n",
    "                                        min_samples_split=16,\n",
    "                                        n_estimators=710))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_ned is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_ned)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307960c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb07389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\",RandomForestClassifier(max_depth=10, min_samples_split=18,\n",
    "                                        n_estimators=910))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_sed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (rf_sed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeedbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8688b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", RandomForestClassifier(max_depth=10, min_samples_leaf=3,\n",
    "                                        min_samples_split=16,\n",
    "                                        n_estimators=710))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for rf_fed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (logistic rf_fed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c0278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78a8648a",
   "metadata": {},
   "source": [
    "## Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=10))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_fd is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_fd)', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f6e5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'm1d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "X = medium_1_data.drop('success',axis=1)\n",
    "y = medium_1_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=1000, fit_prior=False))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_m1d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_m1d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4273bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm2d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_2_features = ['status','category_list','category_groups_list','uuid_p','gender',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_2_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat2\", categorical_2_transformer, categorical_2_features),\n",
    "])\n",
    "\n",
    "\n",
    "X = medium_2_data.drop('success',axis=1)\n",
    "y = medium_2_data['success']\n",
    "\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=1000))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_m2d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_m2d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac6fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run for 'm3d' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_3_features = ['status','category_list','category_groups_list']\n",
    "\n",
    "categorical_3_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat3\", categorical_3_transformer, categorical_3_features),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X = medium_3_data.drop('success',axis=1)\n",
    "y = medium_3_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=10, fit_prior=False))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_m3d is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_m3d)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faa3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'nog' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "no_gender_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "no_gender_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"nog\", no_gender_transformer, no_gender_features),\n",
    "    ])\n",
    "\n",
    "X = no_gender_data.drop('success',axis=1)\n",
    "y = no_gender_data['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=1000, fit_prior=False))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_nog is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_nog)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb480d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'num' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "\n",
    "        (\"num\", numeric_transformer, numeric_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "X = num_data.drop('success',axis=1)\n",
    "y = num_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=10, fit_prior=False))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_num is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_num)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53709987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf358320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'ned' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = no_enrich_df.drop('success',axis=1)\n",
    "y = no_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=10))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_ned is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_ned)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c83a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bae8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'sed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = senti_enrich_df.drop('success',axis=1)\n",
    "y = senti_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=10))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_sed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_sed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0c957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fed' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = follow_enrich_df.drop('success',axis=1)\n",
    "y = follow_enrich_df['success']\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", MultinomialNB(alpha=10))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict \n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# Plot the ROC curve and calculate the AUC metric\n",
    "\n",
    "RocCurveDisplay.from_estimator(clf, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Accuracy score for NB_fed is {clf.score(X_test, y_test)}')\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "\n",
    "# Calculate the classification report \n",
    "print(classification_report(y_test, y_preds))\n",
    "\n",
    "# Calculate the cross validated accuracy \n",
    "cv_acc = cross_val_score(clf,  X, y, cv=5, scoring='accuracy')\n",
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc\n",
    "\n",
    "# Calculate the cross validated precision\n",
    "cv_precision = cross_val_score(clf,  X, y, cv=5, scoring='precision')\n",
    "cv_precision = np.mean(cv_precision)\n",
    "cv_precision\n",
    "\n",
    "# Calculate the cross validated recall \n",
    "cv_recall = cross_val_score(clf,  X, y, cv=5, scoring='recall')\n",
    "cv_recall = np.mean(cv_recall)\n",
    "cv_recall\n",
    "\n",
    "# Calculate the cross validated F1 score\n",
    "cv_f1 = cross_val_score(clf,  X, y, cv=5, scoring='f1')\n",
    "cv_f1 = np.mean(cv_f1)\n",
    "cv_f1\n",
    "\n",
    "# Visualize cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({'Accuracy':cv_acc,\n",
    "                          'Precision': cv_precision,\n",
    "                          'Recall': cv_recall,\n",
    "                          'F1': cv_f1}, index=[0])\n",
    "cv_metrics.T.plot.bar(title='Cross-validated metrics (NB_fed)', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4051b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "272d60fa",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for 'fd' data:\n",
    "\n",
    "# Define preprocessors\n",
    "\n",
    "# pipeline for text data1\n",
    "text1_features = 'short_description_o'\n",
    "text1_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data2\n",
    "text2_features = 'description_o'\n",
    "text2_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for text data3\n",
    "text3_features = 'description_o'\n",
    "text3_transformer = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=\"english\"))\n",
    "])\n",
    "\n",
    "# pipeline for categorical data\n",
    "\n",
    "categorical_features = ['country_code_o', 'status','category_list','category_groups_list','uuid_p','gender','country_code_p',\n",
    "                       'featured_job_title','institution_name','degree_type','subject','is_completed']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not known')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# pipeline for numeric data\n",
    "numeric_features = ['following', 'followers','polarity','subjectivity','rank_o','rank_p','num_events_part','per_exp_at_coy_start',\n",
    "                     'degree_length','employee_count_min','employee_count_max']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"tex1\", text1_transformer, text1_features),\n",
    "        (\"tex2\", text2_transformer, text2_features),\n",
    "        (\"tex3\", text3_transformer, text3_features),\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ])\n",
    "\n",
    "X = full_data.drop('success',axis=1)\n",
    "y = full_data['success']\n",
    "\n",
    "\n",
    "# Split into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make a dictionary to keep model scores\n",
    "model_score = {}\n",
    "best_params = {}\n",
    "\n",
    "# Fit the model to the data\n",
    "clf = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"clf\", LogisticRegression(C=0.23357214690901212, max_iter=150,\n",
    "                                    penalty='l1', solver='liblinear'))])\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1eee8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = clf.preprocessor.named_steps[\"vectorizer\"].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5ef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
